{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "# from torchtext.data.functional import to_map_style_dataset\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# import torchtext.datasets as datasets\n",
    "# import spacy\n",
    "# import GPUtil\n",
    "import warnings\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "# Set to False to skip notebook execution (e.g. for debugging)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "RUN_EXAMPLES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some convenience helper functions used throughout the notebook\n",
    "def is_interactive_notebook():\n",
    "    return __name__ == \"__main__\"\n",
    "\n",
    "def show_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        return fn(*args)\n",
    "\n",
    "def execute_example(fn, args=[]):\n",
    "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
    "        fn(*args)\n",
    "\n",
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "    def step(self):\n",
    "        None\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from turtle import forward\n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architectire. Base for this and many other models\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    \n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequence.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define standard linear + softmax generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax(torch.ones([2,4]), dim=-1)  # dim=-1按最后一维方向来(行方向)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder and Decoder Stacks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoder \n",
    "The encoder is composed of a stack of $N=6$ identical layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![图片](https://github.com/harvardnlp/annotated-transformer/raw/debc9fd747bb2123160a98046ad1c2d4da44a567//images/ModalNet-21.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[copy.deepcopy([1,2]) for _ in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)      # 6层layer堆叠成layers\n",
    "        self.norm = LayerNorm(layer.size)   # LayerNorm\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We employ a residual connection around each of the two sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_2 * \\frac{x-mean}{std+eps} + b_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6): # features是单个特征向量长度\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1,64,512)\n",
    "ln = LayerNorm(512)\n",
    "ln(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2,3,4)\n",
    "mean = x.mean(dim=-1, keepdim=True)\n",
    "mean\n",
    "# keepdim=True时，输出与输入维度相同，仅仅是输出在求范数的维度上元素个数变为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Connection between sublayers\n",
    "$LayerNorm(x+Sublayer(x))$ \n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{model}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # 这里的sublayer是在以后调用时才传入的，一般是具有forward方法的module或可以对x进行操作的函数\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 先通过sublayer[0]即attention模块\n",
    "        # lambda匿名函数，可以对x进行attention操作，输出处理后的向量.\n",
    "        # attention模块输入参数为x和mask\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,mask))\n",
    "        # 然后通过PFFN输出encoder的最终结果\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lambda a : a + 10\n",
    "print(x(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoer\n",
    "The decoder is also composed of a stack of $N=6$ identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x,x,x,tgt_mask))\n",
    "        # encoder的输出作为k,v\n",
    "        # decoder第一个attention的输出作为q\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x,m,m,src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False],\n",
       "         [ True,  True, False, False],\n",
       "         [ True,  True,  True, False],\n",
       "         [ True,  True,  True,  True]]])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    return subsequent_mask == 0\n",
    "subsequent_mask(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可视化掩码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_mask():\n",
    "    LS_data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Subsequent Mask\": subsequent_mask(20)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(20)\n",
    "            for x in range(20)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "            alt.X(\"Window:O\"),\n",
    "            alt.Y(\"Masking:O\"),\n",
    "            alt.Color(\"Subsequent Mask:Q\", scale=alt.Scale(scheme=\"viridis\")),\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n",
    "show_example(example_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention \n",
    "We call our particular attention \"Scaled Dot-Product Attention\".The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a $softmax$ function to obtain the weights on the values.\n",
    "\n",
    "![图片](https://github.com/harvardnlp/annotated-transformer/raw/debc9fd747bb2123160a98046ad1c2d4da44a567//images/ModalNet-19.png)\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. We compute the matrix of outputs as:\n",
    "$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "通过除以$\\sqrt{d_k}$规整dot products的结果是为了防止点积结果太大导致softmax结果太大，最终导致某些区域梯度趋于0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # 注意力分数\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn(3,512)    # mxn\n",
    "k = torch.randn(3,512)    # mxn\n",
    "v = torch.randn(3,256)    # mxk\n",
    "output, attn_scores = attention(q,k,v)  # attention输出矩阵mxk, 注意力得分矩阵mxm\n",
    "output.shape, attn_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head attention\n",
    "多头注意力机制允许模型从不同的特征表示子空间中的不同位置注意到不同的信息，语义信息更为丰富。 \n",
    "多头注意力机制就是先把输入向量投影到$d_{model}/head_num$维，然后通过attentio模块输出$d_{model}/head_num$维的向量，最后再在通道维度上concat还原为$d_{model}$维。\n",
    "![图片](https://github.com/harvardnlp/annotated-transformer/raw/debc9fd747bb2123160a98046ad1c2d4da44a567//images/ModalNet-20.png)\n",
    "\n",
    "形式化表达如下：\n",
    "$Multihead(Q,K,V) = concat(head_1, head_2, ..., head_h)W^O, where \\ head_i = Attention(QW^Q_i, KW^K_i, VW^V_i)$ \n",
    "\n",
    "其中投影矩阵的尺寸分别为$W^Q_i\\in{R^{d_{model}\\times{d_k}}}$, $W^K_i\\in{R^{d_{model}\\times{d_k}}}$, $W^V_i\\in{R^{d_{model}\\times{d_v}}}$, $W^O_i\\in{R^{hd_v\\times{d_{model}}}}$ \n",
    "\n",
    "我们使用$h=8$(8个头)，对于每个head，我们使用$d_k=d_v=d_{model}/h=64$\n",
    "\n",
    "多头注意力的计算参数量和单头差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0  # 在表达式条件为false时触发异常\n",
    "        # 保证d_k永远等于d_v\n",
    "        self.d_k = d_model // h  # d_k = 512 / 8 = 64\n",
    "        self.h = h               # h = 8\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            # -1表示其他维数都明确知道数值，但该维度不确定，让程序自己决定\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # query, key, value = [\n",
    "        #     lin(x).view(nbatches, 8, -1, 64)\n",
    "        #     for lin, x in zip(self.linears, (query, key, value))\n",
    "        # ]\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = (x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k))\n",
    "        del query  # 删除变量，减小内存消耗\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 测试一下Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linears = clones(nn.Linear(512, 512), 4)\n",
    "query = torch.randn(1,3,512)\n",
    "key = torch.randn(1,3,512)\n",
    "value = torch.randn(1,3,512)\n",
    "nbatches = query.size(0)\n",
    "print(\"query经过线性投影后的大小\",linears[0](query).view(1, 8, -1, 64).shape)\n",
    "query, key, value = [\n",
    "            lin(x).view(nbatches, 8, -1, 64)\n",
    "            for lin, x in zip(linears, (query, key, value))\n",
    "        ]\n",
    "print(\"线性投影后的qkv大小:\",query.shape, key.shape, value.shape)\n",
    "x, attn_scores = attention(query, key, value, mask=None, dropout=nn.Dropout())\n",
    "print(\"多头注意力输出向量大小:\",x.shape, attn_scores.shape)\n",
    "x = x.view(nbatches, -1, 512)\n",
    "print(\"concat后的向量大小:\",x.shape)\n",
    "print(\"linear层后的向量大小:\",linears[-1](x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-wise Feed-Forward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements PFFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)  # 512x2048\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)  # 2048x512\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings and Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(5000, 512)\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])  # 2个句子\n",
    "x(segments).shape  # 2个句子，每个句子用8个1x512的向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding\n",
    "$PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$\n",
    "$PE(pos,2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})$ \n",
    "\n",
    "其中i=0,1,2,...,255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)  # 训练时不会更新pe参数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = torch.zeros(5000, 512)\n",
    "position = torch.arange(0, 5000).unsqueeze(1) # 在第1维升维\n",
    "div_term = torch.exp(torch.arange(0, 512, 2) * -(math.log(10000.0) / 512)) # 1x256\n",
    "pe[:, 0::2] = torch.sin(position * div_term) # 5000行，每行的偶数维替换为sin\n",
    "pe[:, 1::2] = torch.cos(position * div_term) # 5000行，每行的奇数维替换为cos\n",
    "pe = pe.unsqueeze(0)  # 在第0维升维\n",
    "pe[:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_positional():\n",
    "    pe = PositionalEncoding(20, 0)\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "    data = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"embedding\": y[0, :, dim],\n",
    "                    \"dimension\": dim,\n",
    "                    \"position\": list(range(100)),\n",
    "                }\n",
    "            )\n",
    "            for dim in [4, 5, 6, 7]\n",
    "        ]\n",
    "    )\n",
    "    return (\n",
    "        alt.Chart(data)\n",
    "        .mark_line()\n",
    "        .properties(width=800)\n",
    "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
    "        .interactive()\n",
    "    )\n",
    "show_example(example_positional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "    # This was important from their code.\n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(1)\n",
      "tensor([[0, 1]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0],\n",
      "         [1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(2)\n",
      "tensor([[0, 1, 2]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0],\n",
      "         [1, 1, 0],\n",
      "         [1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(7)\n",
      "tensor([[0, 1, 2, 7]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0],\n",
      "         [1, 1, 0, 0],\n",
      "         [1, 1, 1, 0],\n",
      "         [1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5, 5]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5, 5, 5]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5, 5, 5, 5]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5, 5, 5, 5, 5]]) \n",
      "-----------------------\n",
      "tensor([[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "         [1, 1, 1, 1, 1, 1, 1, 1, 1]]])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 11])\n",
      "tensor(5)\n",
      "tensor([[0, 1, 2, 7, 5, 5, 5, 5, 5, 5]]) \n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "test_model = make_model(11, 11, 2) # src_vocab=11, tgt_vocab=11, N=6\n",
    "test_model.eval()\n",
    "src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "memory = test_model.encode(src, src_mask)  # torch.Size([1, 10, 512])\n",
    "ys = torch.zeros(1, 1).type_as(src)\n",
    "for i in range(9):\n",
    "    out = test_model.decode(\n",
    "        memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data)\n",
    "    )\n",
    "    print(subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "    print(out[:, -1].shape)\n",
    "    prob = test_model.generator(out[:, -1])  # 取最后一行的输出作为预测向量(1x512)\n",
    "    print(prob.shape)\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.data[0]\n",
    "    print(next_word)\n",
    "    ys = torch.cat(\n",
    "        [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "    )\n",
    "    print(ys,\"\\n-----------------------\")\n",
    "# print(\"Example Untrained Model Prediction:\", ys)\n",
    "\n",
    "# def run_tests():\n",
    "#     for _ in range(10):\n",
    "#         inference_test()\n",
    "# show_example(run_tests)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 5, 6],\n",
       "       [5, 6, 6]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[[1,2,3],[4,5,6]],[[2,1,3],[5,6,6]]])\n",
    "x[:,-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 ('gluon')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9862ae77e9daaaf9c9239620ed827aad4ce184b3776eb7a3f75df899d88e405b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
